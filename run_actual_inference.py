"""
ì‹¤ì œ LLM ì¶”ë¡  ì‹œê°„ ì¸¡ì •ê¸°

ì‹œë‚˜ë¦¬ì˜¤ CSVë¥¼ ì½ì–´ì„œ ì‹¤ì œ GPUì—ì„œ LLM ì¶”ë¡ ì„ ì‹¤í–‰í•˜ê³ 
PREFILL/DECODE ë‹¨ê³„ë³„ ì‹œê°„ì„ ì •ë°€í•˜ê²Œ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import csv
import time
import torch
import os
import sys
from datetime import datetime
from typing import List, Dict, Tuple
import json

# GPU ë©”ëª¨ë¦¬ ì •ë¦¬ìš©
import gc


class ActualInferenceRunner:
    """ì‹¤ì œ LLM ì¶”ë¡  ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì • í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.current_model = None
        self.current_tokenizer = None
        self.current_model_name = None
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA GPUê°€ í•„ìš”í•©ë‹ˆë‹¤!")
        
        self.device = torch.device('cuda')
        print(f"  ì‚¬ìš© ì¤‘ì¸ GPU: {torch.cuda.get_device_name()}")
        print(f" GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    def load_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ë¡œë“œ (í•„ìš”ì‹œì—ë§Œ)"""
        
        if self.current_model_name == model_name:
            print(f" {model_name} ì´ë¯¸ ë¡œë“œë¨ (ì¬ì‚¬ìš©)")
            return True
        
        # ê¸°ì¡´ ëª¨ë¸ ë©”ëª¨ë¦¬ í•´ì œ
        if self.current_model is not None:
            print(f"  ê¸°ì¡´ ëª¨ë¸ ({self.current_model_name}) ë©”ëª¨ë¦¬ í•´ì œ")
            del self.current_model
            del self.current_tokenizer
            torch.cuda.empty_cache()
            gc.collect()
        
        print(f" {model_name} ëª¨ë¸ ë¡œë”© ì¤‘...")
        
        try:
            # ëª¨ë¸ëª… ë§¤í•‘ (ë¡œì»¬ ê²½ë¡œë„ ì§€ì›)
            model_paths = {
                'LLaMA_3.2_1B': 'meta-llama/Llama-3.2-1B-Instruct',
                'LLaMA_3_8B': 'meta-llama/Llama-3-8B-Instruct',
                'LLaMA_3.1_8B': 'meta-llama/Llama-3.1-8B-Instruct',
                'LLaMA_3_70B': 'meta-llama/Llama-3-70B-Instruct'
            }
            
            if model_name not in model_paths:
                print(f" ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
                return False
            
            model_path = model_paths[model_name]
            
            # Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
            from transformers import AutoTokenizer, AutoModelForCausalLM
            import torch
            
            # í† í¬ë‚˜ì´ì € ë¡œë“œ
            print(f" í† í¬ë‚˜ì´ì € ë¡œë”©...")
            self.current_tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            if self.current_tokenizer.pad_token is None:
                self.current_tokenizer.pad_token = self.current_tokenizer.eos_token
            
            # í˜„ì¬ GPU ë©”ëª¨ë¦¬ í™•ì¸
            memory_free = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated()
            memory_free_gb = memory_free / 1e9
            print(f" ì‚¬ìš© ê°€ëŠ¥í•œ GPU ë©”ëª¨ë¦¬: {memory_free_gb:.1f} GB")
            
            # ëª¨ë¸ ë¡œë“œ (GPU ë©”ëª¨ë¦¬ì— ë§ì¶° ì„¤ì •)
            print(f" ëª¨ë¸ ë¡œë”©...")
            if '70B' in model_name:
                # 70B ëª¨ë¸: ë©”ëª¨ë¦¬ ìµœì í™” í•„ìš”
                if memory_free_gb < 40:  # 40GB ë¯¸ë§Œì´ë©´ 8bit ë¡œë”©
                    print(f"  8bit ì–‘ìí™” ë¡œë”© (ë©”ëª¨ë¦¬ ì ˆì•½)")
                    self.current_model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16,
                        device_map='auto',
                        load_in_8bit=True,
                        trust_remote_code=True
                    )
                else:
                    self.current_model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16,
                        device_map='auto',
                        trust_remote_code=True
                    )
            elif '8B' in model_name:
                # 8B ëª¨ë¸: ëŒ€ë¶€ë¶„ GPUì—ì„œ fp16 ê°€ëŠ¥
                if memory_free_gb < 16:
                    print(f" 8bit ì–‘ìí™” ë¡œë”© (ë©”ëª¨ë¦¬ ì ˆì•½)")
                    self.current_model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16,
                        device_map='auto',
                        load_in_8bit=True,
                        trust_remote_code=True
                    )
                else:
                    self.current_model = AutoModelForCausalLM.from_pretrained(
                        model_path,
                        torch_dtype=torch.float16,
                        device_map='auto',
                        trust_remote_code=True
                    )
            else:
                # 1B ëª¨ë¸: ê°€ë²¼ì›Œì„œ ëŒ€ë¶€ë¶„ ë¬¸ì œì—†ìŒ
                self.current_model = AutoModelForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16,
                    device_map='auto',
                    trust_remote_code=True
                )
            
            self.current_model_name = model_name
            print(f" {model_name} ë¡œë”© ì™„ë£Œ")
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶œë ¥
            memory_used = torch.cuda.memory_allocated() / 1e9
            memory_total = torch.cuda.get_device_properties(0).total_memory / 1e9
            print(f" GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.1f}/{memory_total:.1f} GB ({memory_used/memory_total*100:.1f}%)")
            
            return True
            
        except Exception as e:
            print(f" ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return False
    
    def measure_inference_time(self, input_text: str, max_new_tokens: int, batch_size: int = 1) -> Dict:
        """ì‹¤ì œ ì¶”ë¡  ì‹œê°„ ì¸¡ì • (PREFILL + DECODE ë¶„ë¦¬)"""
        
        if self.current_model is None or self.current_tokenizer is None:
            raise RuntimeError("ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
        
        # ì…ë ¥ í† í¬ë‚˜ì´ì§•
        inputs = self.current_tokenizer(
            input_text, 
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=4096  # ìµœëŒ€ ê¸¸ì´ ì œí•œ
        ).to(self.device)
        
        input_length = inputs.input_ids.shape[1]
        
        # GPU ì›Œë°ì—… (ì²« ì‹¤í–‰ì‹œ ì´ˆê¸°í™” ì‹œê°„ ì œê±°)
        with torch.no_grad():
            _ = self.current_model(**inputs)
        
        torch.cuda.synchronize()  # GPU ë™ê¸°í™”
        
        # PREFILL ë‹¨ê³„ ì¸¡ì • (ì…ë ¥ ì²˜ë¦¬)
        prefill_start = time.perf_counter()
        
        with torch.no_grad():
            # KV cacheë¥¼ í¬í•¨í•œ ì²« forward pass
            outputs = self.current_model(**inputs, use_cache=True)
            past_key_values = outputs.past_key_values
        
        torch.cuda.synchronize()
        prefill_end = time.perf_counter()
        prefill_time_ms = (prefill_end - prefill_start) * 1000
        
        # DECODE ë‹¨ê³„ ì¸¡ì • (í† í° ìƒì„±)
        generated_tokens = []
        decode_times = []
        
        current_input_ids = inputs.input_ids
        current_past_key_values = past_key_values
        
        for i in range(max_new_tokens):
            decode_start = time.perf_counter()
            
            with torch.no_grad():
                # ë‹¤ìŒ í† í° ì˜ˆì¸¡
                outputs = self.current_model(
                    input_ids=current_input_ids[:, -1:],  # ë§ˆì§€ë§‰ í† í°ë§Œ
                    past_key_values=current_past_key_values,
                    use_cache=True
                )
                
                # ë‹¤ìŒ í† í° ì„ íƒ (temperature sampling)
                next_token_logits = outputs.logits[:, -1, :] / 0.8  # temperature
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)
                
                # KV cache ì—…ë°ì´íŠ¸
                current_past_key_values = outputs.past_key_values
                current_input_ids = torch.cat([current_input_ids, next_token], dim=-1)
            
            torch.cuda.synchronize()
            decode_end = time.perf_counter()
            
            decode_time_ms = (decode_end - decode_start) * 1000
            decode_times.append(decode_time_ms)
            generated_tokens.append(next_token.item())
            
            # EOS í† í°ì´ë©´ ì¤‘ë‹¨
            if next_token.item() == self.current_tokenizer.eos_token_id:
                break
        
        # ê²°ê³¼ ì •ë¦¬
        actual_output_tokens = len(generated_tokens)
        total_decode_time_ms = sum(decode_times)
        avg_decode_per_token_ms = total_decode_time_ms / actual_output_tokens if actual_output_tokens > 0 else 0
        total_time_ms = prefill_time_ms + total_decode_time_ms
        
        # ìƒì„±ëœ í…ìŠ¤íŠ¸ ë””ì½”ë”©
        generated_text = self.current_tokenizer.decode(generated_tokens, skip_special_tokens=True)
        
        return {
            'input_length': input_length,
            'actual_output_length': actual_output_tokens,
            'requested_output_length': max_new_tokens,
            
            'prefill_time_ms': round(prefill_time_ms, 2),
            'total_decode_time_ms': round(total_decode_time_ms, 2),
            'avg_decode_per_token_ms': round(avg_decode_per_token_ms, 4),
            'total_time_ms': round(total_time_ms, 2),
            'throughput_tokens_per_sec': round(actual_output_tokens / (total_time_ms / 1000), 2) if total_time_ms > 0 else 0,
            
            'generated_text': generated_text[:200] + '...' if len(generated_text) > 200 else generated_text,
            'decode_times_per_token': [round(t, 4) for t in decode_times]
        }


def load_scenarios(csv_file: str) -> List[Dict]:
    """ì‹œë‚˜ë¦¬ì˜¤ CSV íŒŒì¼ ë¡œë“œ"""
    
    filepath = f'/Users/anchovy-mac/Desktop/calculating/experiment/{csv_file}'
    scenarios = []
    
    with open(filepath, 'r', encoding='utf-8') as file:
        reader = csv.DictReader(file)
        for row in reader:
            row['scenario_id'] = int(row['scenario_id'])
            row['input_length'] = int(row['input_length'])
            row['output_length'] = int(row['output_length'])
            row['batch_size'] = int(row['batch_size'])
            scenarios.append(row)
    
    return scenarios


def generate_input_text(length: int) -> str:
    """ì§€ì •ëœ í† í° ê¸¸ì´ì˜ ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„± (ë‹¨ìˆœ ë°˜ë³µ)"""
    
    # ë‹¨ìˆœí•œ ë°˜ë³µ í…ìŠ¤íŠ¸ ìƒì„± (latencyëŠ” ë‚´ìš©ê³¼ ë¬´ê´€í•˜ë¯€ë¡œ)
    # "a" ë¬¸ìë¥¼ ë°˜ë³µí•˜ì—¬ ëŒ€ëµì ì¸ í† í° ê¸¸ì´ ë§ì¶¤
    base_word = "a"
    
    # ëŒ€ëµ í† í°ë‹¹ 1-2ê°œ ë¬¸ìë¡œ ì¶”ì •í•˜ì—¬ ìƒì„±
    # ì‹¤ì œë¡œëŠ” í† í¬ë‚˜ì´ì €ê°€ ì •í™•íˆ ê³„ì‚°í•˜ì§€ë§Œ, ëŒ€ëµì ìœ¼ë¡œ ë§ì¶¤
    repeated_text = base_word * (length * 2)  # ì—¬ìœ ìˆê²Œ ìƒì„±
    
    return repeated_text


def filter_scenarios_by_current_gpu(scenarios: List[Dict]) -> List[Dict]:
    """í˜„ì¬ GPUì—ì„œ ì‹¤í–‰ ê°€ëŠ¥í•œ ì‹œë‚˜ë¦¬ì˜¤ë§Œ í•„í„°ë§"""
    
    current_gpu_name = torch.cuda.get_device_name()
    current_memory_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
    
    print(f"ğŸ–¥ï¸  í˜„ì¬ GPU: {current_gpu_name} ({current_memory_gb:.1f} GB)")
    
    # GPUë³„ ëŒ€ëµì ì¸ ì´ë¦„ ë§¤ì¹­
    gpu_mapping = {
        'Tesla T4': ['T4'],
        'A10G': ['A10G'], 
        'L40S': ['L40S', 'L40s'],
        'A100': ['A100'],
        'A100-SXM4-40GB': ['A100'],
        'A100-SXM4-80GB': ['A100-SXM4-80GB']
    }
    
    # í˜„ì¬ GPUì— í•´ë‹¹í•˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ ì°¾ê¸°
    matching_gpu_names = []
    for gpu_key, aliases in gpu_mapping.items():
        for alias in aliases:
            if alias in current_gpu_name:
                matching_gpu_names.append(gpu_key)
                break
    
    if not matching_gpu_names:
        print(f"âš ï¸  í˜„ì¬ GPU ({current_gpu_name})ì— ëŒ€í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("ğŸ”§ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        return scenarios
    
    # í•„í„°ë§ëœ ì‹œë‚˜ë¦¬ì˜¤
    filtered_scenarios = []
    for scenario in scenarios:
        if scenario['gpu'] in matching_gpu_names:
            filtered_scenarios.append(scenario)
    
    print(f"âœ… {len(filtered_scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ê°€ í˜„ì¬ GPUì— ì í•©í•©ë‹ˆë‹¤.")
    return filtered_scenarios


def run_scenario_measurements(scenarios: List[Dict], runner: ActualInferenceRunner, max_scenarios: int = None) -> List[Dict]:
    """ì‹œë‚˜ë¦¬ì˜¤ë“¤ì— ëŒ€í•´ ì‹¤ì œ ì¶”ë¡  ì‹œê°„ ì¸¡ì •"""
    
    # í˜„ì¬ GPUì— ë§ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë§Œ í•„í„°ë§
    scenarios = filter_scenarios_by_current_gpu(scenarios)
    
    if max_scenarios:
        scenarios = scenarios[:max_scenarios]
    
    results = []
    total_scenarios = len(scenarios)
    
    print(f"â±ï¸  {total_scenarios}ê°œ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•´ ì‹¤ì œ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì‹œì‘")
    print("-" * 60)
    
    for i, scenario in enumerate(scenarios, 1):
        try:
            print(f"\nğŸ”„ [{i}/{total_scenarios}] ì‹œë‚˜ë¦¬ì˜¤ {scenario['scenario_id']} ì²˜ë¦¬ ì¤‘...")
            print(f"   GPU: {scenario['gpu']}, ëª¨ë¸: {scenario['model']}")
            print(f"   ì…ë ¥: {scenario['input_length']}í† í°, ì¶œë ¥: {scenario['output_length']}í† í°")
            
            # ëª¨ë¸ ë¡œë“œ
            if not runner.load_model(scenario['model']):
                raise Exception(f"ëª¨ë¸ {scenario['model']} ë¡œë”© ì‹¤íŒ¨")
            
            # ì…ë ¥ í…ìŠ¤íŠ¸ ìƒì„±
            input_text = generate_input_text(scenario['input_length'])
            
            # ì‹¤ì œ ì¶”ë¡  ì‹¤í–‰ ë° ì‹œê°„ ì¸¡ì •
            measurement = runner.measure_inference_time(
                input_text=input_text,
                max_new_tokens=scenario['output_length'],
                batch_size=scenario['batch_size']
            )
            
            # ê²°ê³¼ ì €ì¥
            result = {
                'scenario_id': scenario['scenario_id'],
                'gpu': scenario['gpu'],
                'model': scenario['model'],
                'use_case': scenario['use_case'],
                'batch_size': scenario['batch_size'],
                
                # ìš”ì²­ íŒŒë¼ë¯¸í„°
                'requested_input_length': scenario['input_length'],
                'requested_output_length': scenario['output_length'],
                
                # ì‹¤ì œ ì¸¡ì • ê²°ê³¼
                'actual_input_length': measurement['input_length'],
                'actual_output_length': measurement['actual_output_length'],
                
                'measured_prefill_ms': measurement['prefill_time_ms'],
                'measured_decode_per_token_ms': measurement['avg_decode_per_token_ms'],
                'measured_total_decode_ms': measurement['total_decode_time_ms'],
                'measured_total_ms': measurement['total_time_ms'],
                'measured_throughput_tokens_per_sec': measurement['throughput_tokens_per_sec'],
                
                'generated_text_preview': measurement['generated_text'],
                'measurement_timestamp': datetime.now().isoformat()
            }
            
            results.append(result)
            
            print(f"   âœ… ì™„ë£Œ: {measurement['total_time_ms']:.1f}ms "
                  f"(PREFILL: {measurement['prefill_time_ms']:.1f}ms, "
                  f"DECODE: {measurement['avg_decode_per_token_ms']:.2f}ms/token)")
            
        except Exception as e:
            print(f"   âŒ ì‹¤íŒ¨: {e}")
            
            # ì‹¤íŒ¨ ê¸°ë¡
            error_result = {
                'scenario_id': scenario['scenario_id'],
                'gpu': scenario['gpu'],
                'model': scenario['model'],
                'use_case': scenario['use_case'],
                'error': str(e),
                'measurement_timestamp': datetime.now().isoformat()
            }
            results.append(error_result)
    
    return results


def save_measurements_to_csv(measurements: List[Dict], filename: str):
    """ì¸¡ì • ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥"""
    
    filepath = f'/Users/anchovy-mac/Desktop/calculating/experiment/{filename}'
    
    if not measurements:
        print("âŒ ì €ì¥í•  ì¸¡ì • ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    fieldnames = list(measurements[0].keys())
    
    with open(filepath, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(measurements)
    
    print(f"ğŸ’¾ ì¸¡ì • ê²°ê³¼ê°€ {filepath}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    print("â±ï¸  LLM ì‹¤ì œ ì¶”ë¡  ì‹œê°„ ì¸¡ì • ì‹œìŠ¤í…œ")
    print("=" * 60)
    
    try:
        # ì¶”ë¡  ì‹¤í–‰ê¸° ì´ˆê¸°í™”
        runner = ActualInferenceRunner()
        
        # í˜„ì¬ GPU ì •ë³´ í™•ì¸
        current_gpu = torch.cuda.get_device_name()
        print(f"\nğŸ–¥ï¸  í˜„ì¬ GPU: {current_gpu}")
        
        # ì‹œë‚˜ë¦¬ì˜¤ íŒŒì¼ ì„ íƒ
        scenario_files = [
            ('length_combination_scenarios.csv', 'length_combination_measurements.csv')
        ]
        
        for scenario_file, output_file in scenario_files:
            print(f"\nğŸ“‹ {scenario_file} ì²˜ë¦¬ ì¤‘...")
            
            try:
                scenarios = load_scenarios(scenario_file)
                print(f"ğŸ“– {len(scenarios)}ê°œ ì‹œë‚˜ë¦¬ì˜¤ ë¡œë“œë¨")
                
                # ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
                measurements = run_scenario_measurements(
                    scenarios, runner, max_scenarios=None  # ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰
                )
                
                save_measurements_to_csv(measurements, output_file)
                
                # ê°„ë‹¨í•œ í†µê³„
                successful = [m for m in measurements if 'error' not in m]
                if successful:
                    avg_time = sum(m['measured_total_ms'] for m in successful) / len(successful)
                    print(f"ğŸ“Š í‰ê·  ì¶”ë¡  ì‹œê°„: {avg_time:.1f}ms")
                
            except FileNotFoundError:
                print(f"âš ï¸  {scenario_file}ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"âŒ {scenario_file} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        
        print(f"\nğŸ‰ ì‹¤ì œ ì¸¡ì • ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        print("ğŸ’¡ CUDA GPUì™€ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")


if __name__ == "__main__":
    main()